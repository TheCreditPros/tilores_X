# ğŸ”§ Manual Model Setup Guide - Open WebUI

## âœ… **You're Logged In - Ready to Configure!**

**Credentials Used**: damon@thecreditpros.com / Credit@123

---

## ğŸš€ **Step-by-Step Model Configuration**

### **Step 1: Navigate to Settings**

1. Look for the **Settings** icon (âš™ï¸ gear icon) in the left sidebar
2. Click on **Settings**
3. Navigate to **Models** or **Connections** section

### **Step 2: Add New Model Connection**

Look for **"Add Model"**, **"New Connection"**, or **"+"** button

### **Step 3: Configure Each Model (Use Same Settings for All)**

**ğŸ”‘ Universal Settings (for ALL 9 models):**

```
Base URL: http://host.docker.internal:8080
API Key: dummy
Provider: OpenAI Compatible
```

---

## ğŸ“‹ **All 9 Models to Add**

### **ğŸ¤– OpenAI Models (3)**

**Model 1:**

```
Model ID: gpt-4o-mini
Display Name: Tilores GPT-4o Mini
Base URL: http://host.docker.internal:8080
API Key: dummy
```

**Model 2:**

```
Model ID: gpt-4o
Display Name: Tilores GPT-4o
Base URL: http://host.docker.internal:8080
API Key: dummy
```

**Model 3:**

```
Model ID: gpt-3.5-turbo
Display Name: Tilores GPT-3.5 Turbo
Base URL: http://host.docker.internal:8080
API Key: dummy
```

### **ğŸ§  Google Gemini Models (4)**

**Model 4:**

```
Model ID: gemini-1.5-flash
Display Name: Tilores Gemini 1.5 Flash
Base URL: http://host.docker.internal:8080
API Key: dummy
```

**Model 5:**

```
Model ID: gemini-1.5-pro
Display Name: Tilores Gemini 1.5 Pro
Base URL: http://host.docker.internal:8080
API Key: dummy
```

**Model 6:**

```
Model ID: gemini-2.0-flash-exp
Display Name: Tilores Gemini 2.0 Flash Experimental
Base URL: http://host.docker.internal:8080
API Key: dummy
```

**Model 7:**

```
Model ID: gemini-2.5-flash
Display Name: Tilores Gemini 2.5 Flash
Base URL: http://host.docker.internal:8080
API Key: dummy
```

### **âš¡ Groq Models (2)**

**Model 8:**

```
Model ID: llama-3.3-70b-versatile
Display Name: Tilores Llama 3.3 70B Versatile
Base URL: http://host.docker.internal:8080
API Key: dummy
```

**Model 9:**

```
Model ID: deepseek-r1-distill-llama-70b
Display Name: Tilores DeepSeek R1 Distill Llama 70B
Base URL: http://host.docker.internal:8080
API Key: dummy
```

---

## ğŸ¯ **After Adding All Models**

### **Set Default Model**

- Go to **Settings > General** or **Preferences**
- Set **Default Model**: `gpt-4o-mini` (recommended)

### **Test Configuration**

Try this query with different models:

```
What is the account status for e.j.price1986@gmail.com?
```

**Expected Response:**

- âœ… "Active" status
- âœ… Customer name "Esteban Price"
- âœ… Product information

---

## ğŸ” **Troubleshooting**

### **If Models Don't Appear:**

- Check that Base URL is exactly: `http://host.docker.internal:8080`
- Ensure API Key is: `dummy`
- Verify Tilores API is running: `curl http://localhost:8080/health`

### **If Models Don't Respond:**

- Test Tilores API directly:
  ```bash
  curl -X POST http://localhost:8080/v1/chat/completions \
    -H 'Content-Type: application/json' \
    -d '{"model":"gpt-4o-mini","messages":[{"role":"user","content":"test"}]}'
  ```

### **Connection Issues:**

- Restart Open WebUI container: `docker restart openwebui`
- Check Docker network: `docker network ls`

---

## ğŸŠ **Success Criteria**

When complete, you should have:

- âœ… **9 models** configured and selectable
- âœ… **3 providers** represented (OpenAI, Google, Groq)
- âœ… **Real Tilores data** returned from test queries
- âœ… **Model comparison** capability across providers

**You're now ready for comprehensive team evaluation of all Tilores models! ğŸš€**
