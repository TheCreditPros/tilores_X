#!/usr/bin/env python3
"""
Phase 3: Continuous Improvement Engine for tilores_X Multi-Spectrum Framework.

This module implements automated quality monitoring and alerting system with
self-improving prompt optimization that learns from previous iterations and
accumulates learning across optimization cycles. Creates continuous improvement
engine with 90% quality threshold monitoring and self-healing optimization.

Key Features:
- Automated quality monitoring with 90% threshold detection
- Real-time alerting system for quality degradation
- Learning accumulation across optimization cycles
- Self-improving prompt optimization with iteration learning
- Automated improvement deployment system
- Self-healing optimization cycles
- Integration with existing quality metrics collector and LangSmith

Author: Roo (Elite Software Engineer)
Created: 2025-08-16
Phase: 3 - Continuous Improvement
"""

import asyncio
import json
import logging
import time
from collections import deque
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from enum import Enum
from pathlib import Path
from typing import Any, Dict, List, Optional

# External dependencies with graceful fallback
try:
    import numpy as np
    NUMPY_AVAILABLE = True
except ImportError:
    NUMPY_AVAILABLE = False

try:
    from langsmith import Client
    LANGSMITH_AVAILABLE = True
except ImportError:
    LANGSMITH_AVAILABLE = False

try:
    from langchain_openai import ChatOpenAI
    LANGCHAIN_AVAILABLE = True
except ImportError:
    LANGCHAIN_AVAILABLE = False

# Import existing frameworks
try:
    from quality_metrics_collector import QualityMetricsCollector
    from phase2_ai_prompt_optimization import Phase2OptimizationOrchestrator
    from virtuous_cycle_framework import VirtuousCycleOrchestrator
    FRAMEWORKS_AVAILABLE = True
except ImportError:
    FRAMEWORKS_AVAILABLE = False
    logging.warning("Existing frameworks not available, using mock implementations")


class AlertSeverity(Enum):
    """Alert severity levels for quality monitoring."""

    CRITICAL = "critical"  # Quality < 85%
    HIGH = "high"         # Quality < 90%
    MEDIUM = "medium"     # Quality declining trend
    LOW = "low"           # Quality variance high
    INFO = "info"         # Quality improvements


class AlertType(Enum):
    """Types of quality alerts."""

    QUALITY_THRESHOLD_BREACH = "quality_threshold_breach"
    QUALITY_DEGRADATION = "quality_degradation"
    HIGH_VARIANCE = "high_variance"
    MODEL_FAILURE = "model_failure"
    SPECTRUM_UNDERPERFORMANCE = "spectrum_underperformance"
    OPTIMIZATION_OPPORTUNITY = "optimization_opportunity"
    IMPROVEMENT_SUCCESS = "improvement_success"


class ImprovementStrategy(Enum):
    """Strategies for continuous improvement."""

    IMMEDIATE_OPTIMIZATION = "immediate_optimization"
    GRADUAL_ENHANCEMENT = "gradual_enhancement"
    PATTERN_REINFORCEMENT = "pattern_reinforcement"
    LEARNING_ACCUMULATION = "learning_accumulation"
    SELF_HEALING = "self_healing"


@dataclass
class QualityAlert:
    """Represents a quality monitoring alert."""

    alert_id: str
    alert_type: AlertType
    severity: AlertSeverity
    spectrum: str
    model: Optional[str]
    current_quality: float
    threshold: float
    message: str
    timestamp: str
    metadata: Dict[str, Any] = field(default_factory=dict)
    resolved: bool = False
    resolution_time: Optional[str] = None


@dataclass
class LearningPattern:
    """Represents accumulated learning from optimization cycles."""

    pattern_id: str
    pattern_type: str
    success_count: int
    failure_count: int
    average_improvement: float
    applicable_contexts: List[str]
    learned_optimizations: List[Dict[str, Any]]
    confidence_score: float
    last_updated: str
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class OptimizationCycleMemory:
    """Memory of optimization cycles for learning accumulation."""

    cycle_id: str
    timestamp: str
    phase: str
    strategies_used: List[str]
    improvements_achieved: Dict[str, float]
    patterns_discovered: List[str]
    success_rate: float
    lessons_learned: List[str]
    next_cycle_recommendations: List[str]
    metadata: Dict[str, Any] = field(default_factory=dict)


class QualityThresholdMonitor:
    """Monitors quality metrics against configurable thresholds."""

    def __init__(self, quality_collector):
        """Initialize the threshold monitor."""
        self.quality_collector = quality_collector
        self.thresholds = {
            'critical': 0.85,  # 85% - Critical threshold
            'warning': 0.90,   # 90% - Warning threshold
            'target': 0.95,    # 95% - Target threshold
            'excellent': 0.98  # 98% - Excellence threshold
        }

        # Monitoring configuration
        self.monitoring_window_hours = 2
        self.trend_analysis_points = 10
        self.variance_threshold = 0.05

        # Alert tracking
        self.active_alerts: Dict[str, QualityAlert] = {}
        self.alert_history: deque = deque(maxlen=1000)

        # Setup logging
        self.logger = logging.getLogger(__name__)

    def check_quality_thresholds(self, spectrum: str) -> List[QualityAlert]:
        """Check quality thresholds for a specific spectrum."""
        alerts = []

        # Get recent metrics for spectrum
        recent_metrics = self.quality_collector.storage.get_spectrum_metrics(
            spectrum, limit=self.trend_analysis_points
        )

        if not recent_metrics:
            return alerts

        # Calculate current quality statistics
        scores = [m.score for m in recent_metrics]
        current_quality = sum(scores) / len(scores)

        # Check threshold breaches
        if current_quality < self.thresholds['critical']:
            alert = self._create_threshold_alert(
                spectrum, current_quality, self.thresholds['critical'],
                AlertSeverity.CRITICAL, "Quality below critical threshold"
            )
            alerts.append(alert)
        elif current_quality < self.thresholds['warning']:
            alert = self._create_threshold_alert(
                spectrum, current_quality, self.thresholds['warning'],
                AlertSeverity.HIGH, "Quality below warning threshold"
            )
            alerts.append(alert)

        # Check for declining trends
        if len(scores) >= 5:
            trend_alert = self._check_quality_trend(spectrum, scores)
            if trend_alert:
                alerts.append(trend_alert)

        # Check for high variance
        if len(scores) >= 3:
            variance_alert = self._check_quality_variance(spectrum, scores)
            if variance_alert:
                alerts.append(variance_alert)

        return alerts

    def _create_threshold_alert(self, spectrum: str, current: float,
                              threshold: float, severity: AlertSeverity,
                              message: str) -> QualityAlert:
        """Create a threshold breach alert."""
        alert_id = f"threshold_{spectrum}_{int(time.time())}"

        return QualityAlert(
            alert_id=alert_id,
            alert_type=AlertType.QUALITY_THRESHOLD_BREACH,
            severity=severity,
            spectrum=spectrum,
            model=None,
            current_quality=current,
            threshold=threshold,
            message=f"{message}: {current:.1%} < {threshold:.1%}",
            timestamp=datetime.now().isoformat(),
            metadata={
                'threshold_type': 'critical' if severity == AlertSeverity.CRITICAL else 'warning',
                'breach_amount': threshold - current
            }
        )

    def _check_quality_trend(self, spectrum: str,
                           scores: List[float]) -> Optional[QualityAlert]:
        """Check for declining quality trends."""
        if len(scores) < 5:
            return None

        # Calculate trend using simple linear regression
        if NUMPY_AVAILABLE:
            x = np.arange(len(scores))
            trend_slope = np.polyfit(x, scores, 1)[0]
        else:
            # Fallback calculation
            n = len(scores)
            x_vals = list(range(n))
            sum_x = sum(x_vals)
            sum_y = sum(scores)
            sum_xy = sum(x_vals[i] * scores[i] for i in range(n))
            sum_x2 = sum(x * x for x in x_vals)

            trend_slope = ((n * sum_xy - sum_x * sum_y) /
                          (n * sum_x2 - sum_x ** 2))

        # Alert if declining trend
        if trend_slope < -0.01:  # 1% decline per measurement
            alert_id = f"trend_{spectrum}_{int(time.time())}"

            return QualityAlert(
                alert_id=alert_id,
                alert_type=AlertType.QUALITY_DEGRADATION,
                severity=AlertSeverity.MEDIUM,
                spectrum=spectrum,
                model=None,
                current_quality=scores[-1],
                threshold=self.thresholds['warning'],
                message=f"Declining quality trend detected: {trend_slope:.3f} per measurement",
                timestamp=datetime.now().isoformat(),
                metadata={
                    'trend_slope': trend_slope,
                    'measurements_analyzed': len(scores),
                    'trend_direction': 'declining'
                }
            )

        return None

    def _check_quality_variance(self, spectrum: str,
                              scores: List[float]) -> Optional[QualityAlert]:
        """Check for high quality variance."""
        if NUMPY_AVAILABLE:
            variance = np.std(scores)
        else:
            mean_score = sum(scores) / len(scores)
            variance = (sum((x - mean_score) ** 2 for x in scores) / len(scores)) ** 0.5

        if variance > self.variance_threshold:
            alert_id = f"variance_{spectrum}_{int(time.time())}"

            return QualityAlert(
                alert_id=alert_id,
                alert_type=AlertType.HIGH_VARIANCE,
                severity=AlertSeverity.LOW,
                spectrum=spectrum,
                model=None,
                current_quality=sum(scores) / len(scores),
                threshold=self.variance_threshold,
                message=f"High quality variance detected: {variance:.3f}",
                timestamp=datetime.now().isoformat(),
                metadata={
                    'variance': variance,
                    'measurements_analyzed': len(scores),
                    'variance_threshold': self.variance_threshold
                }
            )

        return None


class AutomatedAlertingSystem:
    """Automated alerting system for quality degradation."""

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        """Initialize the alerting system."""
        self.config = config or {}
        self.alert_channels = self._initialize_alert_channels()
        self.alert_rules = self._initialize_alert_rules()
        self.alert_history: deque = deque(maxlen=10000)

        # Rate limiting for alerts
        self.alert_cooldown = timedelta(minutes=15)
        self.last_alert_times: Dict[str, datetime] = {}

        self.logger = logging.getLogger(__name__)

    def _initialize_alert_channels(self) -> Dict[str, Any]:
        """Initialize alert delivery channels."""
        return {
            'console': {'enabled': True, 'level': 'INFO'},
            'file': {
                'enabled': True,
                'path': 'tests/speed_experiments/alerts.log',
                'level': 'WARNING'
            },
            'email': {
                'enabled': self.config.get('email_alerts', False),
                'smtp_server': self.config.get('smtp_server', 'localhost'),
                'smtp_port': self.config.get('smtp_port', 587),
                'recipients': self.config.get('alert_recipients', [])
            }
        }

    def _initialize_alert_rules(self) -> Dict[str, Any]:
        """Initialize alerting rules and escalation policies."""
        return {
            'critical': {
                'immediate_notification': True,
                'escalation_delay': timedelta(minutes=5),
                'max_escalations': 3,
                'channels': ['console', 'file', 'email']
            },
            'high': {
                'immediate_notification': True,
                'escalation_delay': timedelta(minutes=15),
                'max_escalations': 2,
                'channels': ['console', 'file']
            },
            'medium': {
                'immediate_notification': False,
                'escalation_delay': timedelta(hours=1),
                'max_escalations': 1,
                'channels': ['console', 'file']
            },
            'low': {
                'immediate_notification': False,
                'escalation_delay': timedelta(hours=4),
                'max_escalations': 0,
                'channels': ['file']
            }
        }

    async def process_alert(self, alert: QualityAlert) -> bool:
        """Process and deliver an alert."""
        # Check rate limiting
        alert_key = f"{alert.spectrum}_{alert.alert_type.value}"
        if self._is_rate_limited(alert_key):
            self.logger.debug(f"Alert rate limited: {alert_key}")
            return False

        # Update rate limiting
        self.last_alert_times[alert_key] = datetime.now()

        # Add to history
        self.alert_history.append(alert)

        # Get alert rules
        rules = self.alert_rules.get(alert.severity.value, {})
        channels = rules.get('channels', ['console'])

        # Deliver alert through configured channels
        success = True
        for channel in channels:
            try:
                await self._deliver_alert(alert, channel)
            except Exception as e:
                self.logger.error(f"Failed to deliver alert via {channel}: {e}")
                success = False

        # Log alert processing
        self.logger.info(f"Alert processed: {alert.alert_id} - {alert.message}")

        return success

    def _is_rate_limited(self, alert_key: str) -> bool:
        """Check if alert is rate limited."""
        if alert_key not in self.last_alert_times:
            return False

        time_since_last = datetime.now() - self.last_alert_times[alert_key]
        return time_since_last < self.alert_cooldown

    async def _deliver_alert(self, alert: QualityAlert, channel: str):
        """Deliver alert through specific channel."""
        if channel == 'console':
            await self._deliver_console_alert(alert)
        elif channel == 'file':
            await self._deliver_file_alert(alert)
        elif channel == 'email':
            await self._deliver_email_alert(alert)

    async def _deliver_console_alert(self, alert: QualityAlert):
        """Deliver alert to console."""
        severity_icons = {
            AlertSeverity.CRITICAL: "ðŸš¨",
            AlertSeverity.HIGH: "âš ï¸",
            AlertSeverity.MEDIUM: "ðŸ“Š",
            AlertSeverity.LOW: "â„¹ï¸"
        }

        icon = severity_icons.get(alert.severity, "ðŸ“¢")
        print(f"\n{icon} QUALITY ALERT [{alert.severity.value.upper()}]")
        print(f"Spectrum: {alert.spectrum}")
        print(f"Message: {alert.message}")
        print(f"Current Quality: {alert.current_quality:.1%}")
        print(f"Threshold: {alert.threshold:.1%}")
        print(f"Time: {alert.timestamp}")
        print("-" * 50)

    async def _deliver_file_alert(self, alert: QualityAlert):
        """Deliver alert to log file."""
        log_path = Path(self.alert_channels['file']['path'])
        log_path.parent.mkdir(parents=True, exist_ok=True)

        log_entry = {
            'timestamp': alert.timestamp,
            'alert_id': alert.alert_id,
            'severity': alert.severity.value,
            'type': alert.alert_type.value,
            'spectrum': alert.spectrum,
            'model': alert.model,
            'current_quality': alert.current_quality,
            'threshold': alert.threshold,
            'message': alert.message,
            'metadata': alert.metadata
        }

        with open(log_path, 'a') as f:
            f.write(json.dumps(log_entry) + '\n')

    async def _deliver_email_alert(self, alert: QualityAlert):
        """Deliver alert via email (if configured)."""
        if not self.alert_channels['email']['enabled']:
            return

        # Email implementation would go here
        # For now, just log the attempt
        self.logger.info(f"Email alert would be sent for: {alert.alert_id}")


class LearningAccumulator:
    """Accumulates learning patterns across optimization cycles."""

    def __init__(self, storage_path: str = "tests/speed_experiments/learning"):
        """Initialize the learning accumulator."""
        self.storage_path = Path(storage_path)
        self.storage_path.mkdir(parents=True, exist_ok=True)

        # Learning storage
        self.patterns_file = self.storage_path / "learning_patterns.json"
        self.cycles_file = self.storage_path / "optimization_cycles.json"

        # In-memory learning data
        self.learning_patterns: Dict[str, LearningPattern] = {}
        self.cycle_memory: List[OptimizationCycleMemory] = []

        # Load existing learning
        self._load_learning_data()

        self.logger = logging.getLogger(__name__)

    def _load_learning_data(self):
        """Load existing learning patterns and cycle memory."""
        # Load learning patterns
        if self.patterns_file.exists():
            try:
                with open(self.patterns_file, 'r') as f:
                    patterns_data = json.load(f)

                for pattern_id, pattern_dict in patterns_data.items():
                    self.learning_patterns[pattern_id] = LearningPattern(
                        pattern_id=pattern_dict['pattern_id'],
                        pattern_type=pattern_dict['pattern_type'],
                        success_count=pattern_dict['success_count'],
                        failure_count=pattern_dict['failure_count'],
                        average_improvement=pattern_dict['average_improvement'],
                        applicable_contexts=pattern_dict['applicable_contexts'],
                        learned_optimizations=pattern_dict['learned_optimizations'],
                        confidence_score=pattern_dict['confidence_score'],
                        last_updated=pattern_dict['last_updated'],
                        metadata=pattern_dict.get('metadata', {})
                    )
            except (json.JSONDecodeError, KeyError) as e:
                self.logger.warning(f"Failed to load learning patterns: {e}")

        # Load cycle memory
        if self.cycles_file.exists():
            try:
                with open(self.cycles_file, 'r') as f:
                    cycles_data = json.load(f)

                for cycle_dict in cycles_data:
                    self.cycle_memory.append(OptimizationCycleMemory(
                        cycle_id=cycle_dict['cycle_id'],
                        timestamp=cycle_dict['timestamp'],
                        phase=cycle_dict['phase'],
                        strategies_used=cycle_dict['strategies_used'],
                        improvements_achieved=cycle_dict['improvements_achieved'],
                        patterns_discovered=cycle_dict['patterns_discovered'],
                        success_rate=cycle_dict['success_rate'],
                        lessons_learned=cycle_dict['lessons_learned'],
                        next_cycle_recommendations=cycle_dict['next_cycle_recommendations'],
                        metadata=cycle_dict.get('metadata', {})
                    ))
            except (json.JSONDecodeError, KeyError) as e:
                self.logger.warning(f"Failed to load cycle memory: {e}")

    def record_optimization_cycle(self, cycle_results: Dict[str, Any]):
        """Record results from an optimization cycle for learning."""
        cycle_memory = OptimizationCycleMemory(
            cycle_id=cycle_results['cycle_id'],
            timestamp=cycle_results.get('timestamp', datetime.now().isoformat()),
            phase=cycle_results.get('phase', 'continuous_improvement'),
            strategies_used=self._extract_strategies_used(cycle_results),
            improvements_achieved=self._extract_improvements(cycle_results),
            patterns_discovered=self._extract_patterns(cycle_results),
            success_rate=self._calculate_cycle_success_rate(cycle_results),
            lessons_learned=self._extract_lessons_learned(cycle_results),
            next_cycle_recommendations=cycle_results.get('next_actions', []),
            metadata=cycle_results.get('metadata', {})
        )

        self.cycle_memory.append(cycle_memory)

        # Update learning patterns
        self._update_learning_patterns(cycle_memory)

        # Persist learning data
        self._save_learning_data()

        self.logger.info(f"Recorded optimization cycle: {cycle_memory.cycle_id}")

    def _extract_strategies_used(self, cycle_results: Dict[str, Any]) -> List[str]:
        """Extract optimization strategies used in the cycle."""
        strategies = []

        # Extract from model strategies
        model_strategies = cycle_results.get('model_strategies', [])
        for strategy in model_strategies:
            approach = strategy.get('optimization_approach', '')
            if approach and approach not in strategies:
                strategies.append(approach)

        # Extract from improvements
        improvements = cycle_results.get('improvements', {})
        for spectrum, improvement in improvements.items():
            strategy = improvement.get('optimization_strategy', '')
            if strategy and strategy not in strategies:
                strategies.append(strategy)

        return strategies

    def _extract_improvements(self, cycle_results: Dict[str, Any]) -> Dict[str, float]:
        """Extract quality improvements achieved."""
        improvements = {}

        cycle_improvements = cycle_results.get('improvements', {})
        for spectrum, improvement in cycle_improvements.items():
            quality_gain = improvement.get('quality_improvement', 0)
            improvements[spectrum] = quality_gain

        return improvements

    def _extract_patterns(self, cycle_results: Dict[str, Any]) -> List[str]:
        """Extract successful patterns discovered."""
        patterns = []

        identified_patterns = cycle_results.get('identified_patterns', [])
        for pattern in identified_patterns:
            pattern_id = pattern.get('pattern_id', '')
            if pattern_id:
                patterns.append(pattern_id)

        return patterns

    def _calculate_cycle_success_rate(self, cycle_results: Dict[str, Any]) -> float:
        """Calculate overall success rate for the cycle."""
        improvements = cycle_results.get('improvements', {})
        if not improvements:
            return 0.0

        successful_improvements = sum(
            1 for improvement in improvements.values()
            if improvement.get('quality_improvement', 0) > 0.01
        )

        return successful_improvements / len(improvements)

    def _extract_lessons_learned(self, cycle_results: Dict[str, Any]) -> List[str]:
        """Extract lessons learned from the cycle."""
        lessons = []

        # Extract from validation results
        validation = cycle_results.get('phases', {}).get('validation', {})
        for spectrum, validation_data in validation.items():
            if isinstance(validation_data, dict):
                if validation_data.get('is_statistically_significant', False):
                    lessons.append(f"Successful optimization pattern for {spectrum}")
                else:
                    lessons.append(f"Optimization approach needs refinement for {spectrum}")

        # Extract from recommendations
        recommendations = cycle_results.get('recommendations', {})
        strategy = recommendations.get('optimization_strategy', '')
        if strategy:
            lessons.append(f"Next cycle should focus on {strategy} strategy")

        return lessons

    def _update_learning_patterns(self, cycle_memory: OptimizationCycleMemory):
        """Update learning patterns based on cycle results."""
        for strategy in cycle_memory.strategies_used:
            pattern_id = f"strategy_{strategy}"

            if pattern_id not in self.learning_patterns:
                self.learning_patterns[pattern_id] = LearningPattern(
                    pattern_id=pattern_id,
                    pattern_type=strategy,
                    success_count=0,
                    failure_count=0,
                    average_improvement=0.0,
                    applicable_contexts=[],
                    learned_optimizations=[],
                    confidence_score=0.0,
                    last_updated=datetime.now().isoformat()
                )

            pattern = self.learning_patterns[pattern_id]

            # Update success/failure counts
            if cycle_memory.success_rate > 0.5:
                pattern.success_count += 1

                # Update average improvement
                total_improvement = sum(cycle_memory.improvements_achieved.values())
                if total_improvement > 0:
                    current_total = pattern.average_improvement * (pattern.success_count - 1)
                    pattern.average_improvement = (current_total + total_improvement) / pattern.success_count
            else:
                pattern.failure_count += 1

            # Update confidence score
            total_attempts = pattern.success_count + pattern.failure_count
            pattern.confidence_score = pattern.success_count / total_attempts if total_attempts > 0 else 0

            # Update applicable contexts
            for spectrum in cycle_memory.improvements_achieved.keys():
                if spectrum not in pattern.applicable_contexts:
                    pattern.applicable_contexts.append(spectrum)

            pattern.last_updated = datetime.now().isoformat()

    def _save_learning_data(self):
        """Save learning patterns and cycle memory to storage."""
        # Save learning patterns
        patterns_data = {}
        for pattern_id, pattern in self.learning_patterns.items():
            patterns_data[pattern_id] = {
                'pattern_id': pattern.pattern_id,
                'pattern_type': pattern.pattern_type,
                'success_count': pattern.success_count,
                'failure_count': pattern.failure_count,
                'average_improvement': pattern.average_improvement,
                'applicable_contexts': pattern.applicable_contexts,
                'learned_optimizations': pattern.learned_optimizations,
                'confidence_score': pattern.confidence_score,
                'last_updated': pattern.last_updated,
                'metadata': pattern.metadata
            }

        with open(self.patterns_file, 'w') as f:
            json.dump(patterns_data, f, indent=2)

        # Save cycle memory
        cycles_data = []
        for cycle in self.cycle_memory[-100:]:  # Keep last 100 cycles
            cycles_data.append({
                'cycle_id': cycle.cycle_id,
                'timestamp': cycle.timestamp,
                'phase': cycle.phase,
                'strategies_used': cycle.strategies_used,
                'improvements_achieved': cycle.improvements_achieved,
                'patterns_discovered': cycle.patterns_discovered,
                'success_rate': cycle.success_rate,
                'lessons_learned': cycle.lessons_learned,
                'next_cycle_recommendations': cycle.next_cycle_recommendations,
                'metadata': cycle.metadata
            })

        with open(self.cycles_file, 'w') as f:
            json.dump(cycles_data, f, indent=2)

    def get_learned_patterns_for_context(self, context: str) -> List[LearningPattern]:
        """Get learned patterns applicable to a specific context."""
        applicable_patterns = []

        for pattern in self.learning_patterns.values():
            if (context in pattern.applicable_contexts and
                pattern.confidence_score > 0.6):
                applicable_patterns.append(pattern)

        # Sort by confidence score
        applicable_patterns.sort(key=lambda p: p.confidence_score, reverse=True)

        return applicable_patterns


class SelfImprovingOptimizer:
    """Self-improving prompt optimizer that learns from iterations."""

    def __init__(self, learning_accumulator: LearningAccumulator):
        """Initialize the self-improving optimizer."""
        self.learning_accumulator = learning_accumulator
        self.optimization_history: deque = deque(maxlen=1000)

        # AI integration for self-improvement
        if LANGCHAIN_AVAILABLE:
            self.improvement_llm = ChatOpenAI(
                model="gpt-4o-mini",
                temperature=0.2
            )
        else:
            self.improvement_llm = None

        self.logger = logging.getLogger(__name__)

    async def optimize_with_learning(self, spectrum: str,
                                   current_quality: float,
                                   context: Dict[str, Any]) -> Dict[str, Any]:
        """Optimize prompts using accumulated learning."""
        self.logger.info(f"ðŸ§  Self-improving optimization for {spectrum}")

        # Get applicable learned patterns
        learned_patterns = self.learning_accumulator.get_learned_patterns_for_context(spectrum)

        # Analyze historical optimization attempts
        historical_context = self._analyze_historical_optimizations(spectrum)

        # Generate learning-informed optimization
        optimization_result = await self._generate_learned_optimization(
            spectrum, current_quality, learned_patterns, historical_context, context
        )

        # Record this optimization attempt
        self._record_optimization_attempt(spectrum, optimization_result)

        return optimization_result

    def _analyze_historical_optimizations(self, spectrum: str) -> Dict[str, Any]:
        """Analyze historical optimization attempts for this spectrum."""
        spectrum_history = [
            opt for opt in self.optimization_history
            if opt.get('spectrum') == spectrum
        ]

        if not spectrum_history:
            return {'no_history': True}

        # Analyze success patterns
        successful_opts = [opt for opt in spectrum_history if opt.get('success', False)]
        failed_opts = [opt for opt in spectrum_history if not opt.get('success', True)]

        analysis = {
            'total_attempts': len(spectrum_history),
            'successful_attempts': len(successful_opts),
            'success_rate': len(successful_opts) / len(spectrum_history),
            'failure_rate': len(failed_opts) / len(spectrum_history),
            'recent_attempts': spectrum_history[-5:],
            'success_patterns': self._extract_success_patterns(successful_opts),
            'failure_patterns': self._extract_failure_patterns(failed_opts)
        }

        return analysis

    def _extract_success_patterns(self, successful_opts: List[Dict[str, Any]]) -> List[str]:
        """Extract patterns from successful optimizations."""
        patterns = []
        for opt in successful_opts:
            strategy = opt.get('strategy', '')
            if strategy and strategy not in patterns:
                patterns.append(strategy)
        return patterns

    def _extract_failure_patterns(self, failed_opts: List[Dict[str, Any]]) -> List[str]:
        """Extract patterns from failed optimizations."""
        patterns = []
        for opt in failed_opts:
            strategy = opt.get('strategy', '')
            if strategy and strategy not in patterns:
                patterns.append(strategy)
        return patterns

    async def _generate_learned_optimization(
        self,
        spectrum: str,
        current_quality: float,
        learned_patterns: List[LearningPattern],
        historical_context: Dict[str, Any],
        context: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Generate optimization using accumulated learning."""
        optimization_start = time.time()

        # Select best learned pattern
        best_pattern = None
        if learned_patterns:
            best_pattern = learned_patterns[0]  # Highest confidence

        # Generate optimization strategy
        if best_pattern and best_pattern.confidence_score > 0.8:
            # Use high-confidence learned pattern

            strategy = best_pattern.pattern_type
            expected_improvement = best_pattern.average_improvement
        elif historical_context.get('success_rate', 0) > 0.7:
            # Use successful historical approach
            strategy = 'pattern_reinforcement'
            expected_improvement = 0.03
        else:
            # Use conservative approach for uncertain cases
            strategy = 'gradual_enhancement'
            expected_improvement = 0.02

        # Generate optimized prompt using learning
        optimized_prompt = await self._generate_prompt_with_learning(
            spectrum, strategy, learned_patterns, context
        )

        optimization_result = {
            'spectrum': spectrum,
            'strategy': strategy,
            'optimized_prompt': optimized_prompt,
            'expected_improvement': expected_improvement,
            'confidence': best_pattern.confidence_score if best_pattern else 0.5,
            'learning_applied': len(learned_patterns),
            'optimization_time': time.time() - optimization_start,
            'metadata': {
                'historical_context': historical_context,
                'learned_patterns_used': [p.pattern_id for p in learned_patterns[:3]],
                'optimization_timestamp': datetime.now().isoformat()
            }
        }

        return optimization_result

    async def _generate_prompt_with_learning(
        self,
        spectrum: str,
        strategy: str,
        learned_patterns: List[LearningPattern],
        context: Dict[str, Any]
    ) -> str:
        """Generate optimized prompt incorporating learning."""
        base_prompt = f"Analyze {spectrum} data for customer insights."

        if self.improvement_llm and learned_patterns:
            # Use AI with learning context
            learning_context = "\n".join([
                f"- {pattern.pattern_type}: {pattern.average_improvement:.1%} improvement"
                for pattern in learned_patterns[:3]
            ])

            prompt_template = f"""
You are an expert prompt engineer with access to learning patterns.

LEARNING CONTEXT:
{learning_context}

OPTIMIZATION TASK:
- Spectrum: {spectrum}
- Strategy: {strategy}
- Current Quality: {context.get('current_quality', 0):.1%}
- Target: 90%+

BASE PROMPT:
{base_prompt}

Generate an optimized prompt that incorporates the successful patterns:
"""

            try:
                response = await self.improvement_llm.ainvoke(prompt_template)
                if hasattr(response, 'content'):
                    content = response.content
                    return content if isinstance(content, str) else str(content)
                return str(response)
            except Exception as e:
                self.logger.warning(f"AI optimization failed: {e}")

        # Fallback optimization using patterns
        if learned_patterns:
            best_pattern = learned_patterns[0]
            optimized_prompt = f"""
You are an expert in {spectrum} analysis with proven optimization patterns.

LEARNED OPTIMIZATION APPROACH: {best_pattern.pattern_type}
SUCCESS RATE: {best_pattern.confidence_score:.1%}
AVERAGE IMPROVEMENT: {best_pattern.average_improvement:.1%}

{base_prompt}

Apply the learned optimization patterns to achieve 90%+ quality.
"""
        else:
            # Basic optimization without learning
            optimized_prompt = f"""
You are an expert in {spectrum} analysis.

OPTIMIZATION STRATEGY: {strategy}
TARGET QUALITY: 90%+

{base_prompt}

Provide comprehensive, accurate analysis with professional insights.
"""

        return optimized_prompt.strip()

    def _record_optimization_attempt(self, spectrum: str, result: Dict[str, Any]):
        """Record optimization attempt for future learning."""
        attempt_record = {
            'spectrum': spectrum,
            'timestamp': datetime.now().isoformat(),
            'strategy': result.get('strategy', ''),
            'expected_improvement': result.get('expected_improvement', 0),
            'confidence': result.get('confidence', 0),
            'success': None,  # Will be updated when results are available
            'metadata': result.get('metadata', {})
        }

        self.optimization_history.append(attempt_record)
        self.logger.info(f"Recorded optimization attempt for {spectrum}")


class AutomatedImprovementDeployment:
    """Automated system for deploying successful improvements."""

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        """Initialize the deployment system."""
        self.config = config or {}
        self.deployment_history: deque = deque(maxlen=500)

        # Deployment thresholds
        self.min_improvement_threshold = 0.02  # 2% minimum improvement
        self.confidence_threshold = 0.8        # 80% confidence required
        self.validation_sample_size = 5        # Tests required for validation

        self.logger = logging.getLogger(__name__)

    async def evaluate_deployment_readiness(
        self,
        optimization_results: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Evaluate if optimization results are ready for deployment."""
        spectrum = optimization_results.get('spectrum', '')
        improvement = optimization_results.get('expected_improvement', 0)
        confidence = optimization_results.get('confidence', 0)

        # Check deployment criteria
        meets_improvement = improvement >= self.min_improvement_threshold
        meets_confidence = confidence >= self.confidence_threshold

        deployment_decision = {
            'ready_for_deployment': meets_improvement and meets_confidence,
            'improvement_check': meets_improvement,
            'confidence_check': meets_confidence,
            'spectrum': spectrum,
            'improvement': improvement,
            'confidence': confidence,
            'recommendation': self._generate_deployment_recommendation(
                meets_improvement, meets_confidence, improvement, confidence
            ),
            'timestamp': datetime.now().isoformat()
        }

        return deployment_decision

    def _generate_deployment_recommendation(
        self,
        meets_improvement: bool,
        meets_confidence: bool,
        improvement: float,
        confidence: float
    ) -> str:
        """Generate deployment recommendation."""
        if meets_improvement and meets_confidence:
            return "DEPLOY - All criteria met"
        elif meets_improvement and not meets_confidence:
            return f"VALIDATE - Good improvement ({improvement:.1%}) but low confidence ({confidence:.1%})"
        elif not meets_improvement and meets_confidence:
            return f"OPTIMIZE - High confidence ({confidence:.1%}) but low improvement ({improvement:.1%})"
        else:
            return f"REJECT - Both improvement ({improvement:.1%}) and confidence ({confidence:.1%}) below thresholds"

    async def deploy_optimization(
        self,
        optimization_results: Dict[str, Any],
        target_system: str = "production"
    ) -> Dict[str, Any]:
        """Deploy optimization to target system."""
        deployment_start = time.time()

        # Evaluate readiness
        readiness = await self.evaluate_deployment_readiness(optimization_results)

        if not readiness['ready_for_deployment']:
            return {
                'deployed': False,
                'reason': readiness['recommendation'],
                'readiness_evaluation': readiness
            }

        # Simulate deployment process
        deployment_result = {
            'deployed': True,
            'deployment_id': f"deploy_{int(time.time())}",
            'spectrum': optimization_results.get('spectrum', ''),
            'target_system': target_system,
            'deployment_time': time.time() - deployment_start,
            'optimized_prompt': optimization_results.get('optimized_prompt', ''),
            'expected_improvement': optimization_results.get('expected_improvement', 0),
            'timestamp': datetime.now().isoformat(),
            'rollback_available': True
        }

        # Record deployment
        self.deployment_history.append(deployment_result)

        self.logger.info(f"Deployed optimization for {deployment_result['spectrum']}")

        return deployment_result


class ContinuousImprovementOrchestrator:
    """Main orchestrator for Phase 3 continuous improvement with self-healing."""

    def __init__(self, quality_collector, config: Optional[Dict[str, Any]] = None):
        """Initialize the continuous improvement orchestrator."""
        self.quality_collector = quality_collector
        self.config = config or {}

        # Initialize core components
        self.threshold_monitor = QualityThresholdMonitor(quality_collector)
        self.alerting_system = AutomatedAlertingSystem(config)
        self.learning_accumulator = LearningAccumulator()
        self.self_improving_optimizer = SelfImprovingOptimizer(self.learning_accumulator)
        self.deployment_system = AutomatedImprovementDeployment(config)

        # Continuous improvement configuration
        self.monitoring_interval = timedelta(minutes=30)
        self.optimization_cooldown = timedelta(hours=2)
        self.max_concurrent_optimizations = 3

        # State tracking
        self.active_optimizations: set = set()
        self.last_optimization_times: Dict[str, datetime] = {}
        self.improvement_cycles: deque = deque(maxlen=100)

        # Integration with existing frameworks
        if FRAMEWORKS_AVAILABLE:
            try:
                self.phase2_orchestrator = Phase2OptimizationOrchestrator()
                if LANGSMITH_AVAILABLE:
                    langsmith_client = Client()
                    self.virtuous_cycle = VirtuousCycleOrchestrator(langsmith_client)
                else:
                    self.virtuous_cycle = None
            except Exception as e:
                self.logger.warning(f"Failed to initialize existing frameworks: {e}")
                self.phase2_orchestrator = None
                self.virtuous_cycle = None
        else:
            self.phase2_orchestrator = None
            self.virtuous_cycle = None

        self.logger = logging.getLogger(__name__)

    async def start_continuous_monitoring(self):
        """Start continuous quality monitoring and improvement."""
        self.logger.info("ðŸš€ Starting Phase 3 Continuous Improvement Engine")

        # Start monitoring loop
        while True:
            try:
                await self._run_monitoring_cycle()
                await asyncio.sleep(self.monitoring_interval.total_seconds())
            except KeyboardInterrupt:
                self.logger.info("Continuous monitoring stopped by user")
                break
            except Exception as e:
                self.logger.error(f"Monitoring cycle failed: {e}")
                await asyncio.sleep(60)  # Wait 1 minute before retry

    async def _run_monitoring_cycle(self):
        """Run a single monitoring and improvement cycle."""
        cycle_start = datetime.now()
        cycle_id = f"continuous_improvement_{int(time.time())}"

        self.logger.info(f"ðŸ” Running monitoring cycle: {cycle_id}")

        # Get all monitored spectrums
        spectrums = self._get_monitored_spectrums()

        cycle_results = {
            'cycle_id': cycle_id,
            'timestamp': cycle_start.isoformat(),
            'spectrums_monitored': len(spectrums),
            'alerts_generated': 0,
            'optimizations_triggered': 0,
            'improvements_deployed': 0,
            'alerts': [],
            'optimizations': [],
            'deployments': []
        }

        # Monitor each spectrum
        for spectrum in spectrums:
            # Check quality thresholds
            alerts = self.threshold_monitor.check_quality_thresholds(spectrum)

            for alert in alerts:
                # Process alert
                await self.alerting_system.process_alert(alert)
                cycle_results['alerts'].append(alert.alert_id)
                cycle_results['alerts_generated'] += 1

                # Trigger optimization if needed
                if alert.severity in [AlertSeverity.CRITICAL, AlertSeverity.HIGH]:
                    optimization_triggered = await self._trigger_optimization(
                        spectrum, alert, cycle_results
                    )
                    if optimization_triggered:
                        cycle_results['optimizations_triggered'] += 1

        # Record cycle
        self.improvement_cycles.append(cycle_results)

        cycle_duration = (datetime.now() - cycle_start).total_seconds()
        self.logger.info(f"âœ… Monitoring cycle completed in {cycle_duration:.1f}s")

        return cycle_results

    def _get_monitored_spectrums(self) -> List[str]:
        """Get list of spectrums to monitor."""
        # Get spectrums from recent metrics
        recent_metrics = self.quality_collector.storage.get_recent_metrics(100)
        spectrums = set()

        for metric in recent_metrics:
            spectrums.add(metric.spectrum)

        # Default spectrums if no recent data
        if not spectrums:
            spectrums = {
                'customer_profile', 'credit_analysis', 'transaction_history',
                'call_center_operations', 'entity_relationship',
                'geographic_analysis', 'temporal_analysis'
            }

        return list(spectrums)

    async def _trigger_optimization(
        self,
        spectrum: str,
        alert: QualityAlert,
        cycle_results: Dict[str, Any]
    ) -> bool:
        """Trigger optimization for a spectrum based on alert."""
        # Check if optimization is already running
        if spectrum in self.active_optimizations:
            self.logger.info(f"Optimization already running for {spectrum}")
            return False

        # Check cooldown period
        if spectrum in self.last_optimization_times:
            time_since_last = datetime.now() - self.last_optimization_times[spectrum]
            if time_since_last < self.optimization_cooldown:
                self.logger.info(f"Optimization cooldown active for {spectrum}")
                return False

        # Check concurrent optimization limit
        if len(self.active_optimizations) >= self.max_concurrent_optimizations:
            self.logger.info("Maximum concurrent optimizations reached")
            return False

        # Start optimization
        self.active_optimizations.add(spectrum)
        self.last_optimization_times[spectrum] = datetime.now()

        try:
            # Run self-improving optimization
            optimization_context = {
                'alert': alert,
                'current_quality': alert.current_quality,
                'severity': alert.severity.value,
                'trigger_reason': alert.message
            }

            optimization_result = await self.self_improving_optimizer.optimize_with_learning(
                spectrum, alert.current_quality, optimization_context
            )

            # Evaluate for deployment
            deployment_decision = await self.deployment_system.evaluate_deployment_readiness(
                optimization_result
            )

            # Deploy if ready
            if deployment_decision['ready_for_deployment']:
                deployment_result = await self.deployment_system.deploy_optimization(
                    optimization_result
                )
                cycle_results['deployments'].append(deployment_result)
                cycle_results['improvements_deployed'] += 1

                self.logger.info(f"âœ… Deployed improvement for {spectrum}")

            cycle_results['optimizations'].append({
                'spectrum': spectrum,
                'optimization_result': optimization_result,
                'deployment_decision': deployment_decision
            })

            return True

        except Exception as e:
            self.logger.error(f"Optimization failed for {spectrum}: {e}")
            return False
        finally:
            # Remove from active optimizations
            self.active_optimizations.discard(spectrum)

    async def run_self_healing_cycle(self) -> Dict[str, Any]:
        """Run a complete self-healing optimization cycle."""
        cycle_start = datetime.now()
        cycle_id = f"self_healing_{int(time.time())}"

        self.logger.info(f"ðŸ”„ Starting self-healing cycle: {cycle_id}")

        # Get all spectrums and their current quality
        spectrums = self._get_monitored_spectrums()
        healing_results = {
            'cycle_id': cycle_id,
            'timestamp': cycle_start.isoformat(),
            'spectrums_analyzed': len(spectrums),
            'healing_actions': [],
            'improvements_achieved': {},
            'learning_applied': 0
        }

        for spectrum in spectrums:
            # Analyze spectrum health
            spectrum_health = await self._analyze_spectrum_health(spectrum)

            if spectrum_health['needs_healing']:
                # Apply self-healing optimization
                healing_action = await self._apply_self_healing(spectrum, spectrum_health)
                healing_results['healing_actions'].append(healing_action)

                if healing_action.get('improvement_achieved', 0) > 0:
                    healing_results['improvements_achieved'][spectrum] = healing_action['improvement_achieved']

        # Record learning from healing cycle
        self.learning_accumulator.record_optimization_cycle(healing_results)
        healing_results['learning_applied'] = len(self.learning_accumulator.learning_patterns)

        cycle_duration = (datetime.now() - cycle_start).total_seconds()
        healing_results['duration'] = cycle_duration

        self.logger.info(f"âœ… Self-healing cycle completed in {cycle_duration:.1f}s")

        return healing_results

    async def _analyze_spectrum_health(self, spectrum: str) -> Dict[str, Any]:
        """Analyze the health of a specific spectrum."""
        # Get recent quality metrics
        recent_metrics = self.quality_collector.storage.get_spectrum_metrics(spectrum, limit=20)

        if not recent_metrics:
            return {'needs_healing': False, 'reason': 'no_data'}

        scores = [m.score for m in recent_metrics]
        current_quality = sum(scores) / len(scores)

        # Calculate health indicators
        health_analysis = {
            'spectrum': spectrum,
            'current_quality': current_quality,
            'measurement_count': len(scores),
            'needs_healing': False,
            'healing_priority': 'low',
            'issues': []
        }

        # Check for quality issues
        if current_quality < 0.90:
            health_analysis['needs_healing'] = True
            health_analysis['healing_priority'] = 'high' if current_quality < 0.85 else 'medium'
            health_analysis['issues'].append('below_quality_threshold')

        # Check for variance issues
        if len(scores) >= 3:
            if NUMPY_AVAILABLE:
                variance = np.std(scores)
            else:
                mean_score = sum(scores) / len(scores)
                variance = (sum((x - mean_score) ** 2 for x in scores) / len(scores)) ** 0.5

            if variance > 0.05:
                health_analysis['needs_healing'] = True
                health_analysis['issues'].append('high_variance')

        return health_analysis

    async def _apply_self_healing(self, spectrum: str, health_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """Apply self-healing optimization to a spectrum."""
        healing_start = time.time()

        # Get learned patterns for this spectrum
        learned_patterns = self.learning_accumulator.get_learned_patterns_for_context(spectrum)

        # Create healing context
        healing_context = {
            'health_analysis': health_analysis,
            'healing_priority': health_analysis['healing_priority'],
            'issues': health_analysis['issues'],
            'current_quality': health_analysis['current_quality']
        }

        # Apply self-improving optimization
        optimization_result = await self.self_improving_optimizer.optimize_with_learning(
            spectrum, health_analysis['current_quality'], healing_context
        )

        # Simulate testing the healing
        healing_effectiveness = await self._test_healing_effectiveness(
            spectrum, optimization_result
        )

        healing_action = {
            'spectrum': spectrum,
            'healing_strategy': optimization_result.get('strategy', ''),
            'optimization_applied': True,
            'improvement_achieved': healing_effectiveness.get('improvement', 0),
            'healing_time': time.time() - healing_start,
            'learned_patterns_used': len(learned_patterns),
            'metadata': {
                'health_issues': health_analysis['issues'],
                'optimization_result': optimization_result,
                'healing_effectiveness': healing_effectiveness
            }
        }

        return healing_action

    async def _test_healing_effectiveness(
        self,
        spectrum: str,
        optimization_result: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Test the effectiveness of applied healing."""
        # Simulate testing the optimized prompt
        # In production, this would run actual tests

        baseline_quality = optimization_result.get('metadata', {}).get('current_quality', 0.85)
        expected_improvement = optimization_result.get('expected_improvement', 0.02)

        # Simulate realistic improvement with some variance
        actual_improvement = expected_improvement * (0.8 + 0.4 * (time.time() % 1))
        new_quality = min(0.999, baseline_quality + actual_improvement)

        return {
            'baseline_quality': baseline_quality,
            'new_quality': new_quality,
            'improvement': actual_improvement,
            'effectiveness_score': actual_improvement / expected_improvement if expected_improvement > 0 else 1.0,
            'test_timestamp': datetime.now().isoformat()
        }


# Main execution function for testing
async def main():
    """Main function to demonstrate Phase 3 continuous improvement."""
    logging.basicConfig(level=logging.INFO,
                       format='%(asctime)s - %(levelname)s - %(message)s')

    # Initialize quality collector (mock for demo)
    if FRAMEWORKS_AVAILABLE:
        quality_collector = QualityMetricsCollector()
    else:
        # Mock quality collector for demo
        class MockQualityCollector:
            def __init__(self):
                self.storage = type('MockStorage', (), {
                    'get_spectrum_metrics': lambda self, spectrum, limit=10: [],
                    'get_recent_metrics': lambda self, limit=100: []
                })()

        quality_collector = MockQualityCollector()

    # Create continuous improvement orchestrator
    orchestrator = ContinuousImprovementOrchestrator(quality_collector)

    try:
        print("ðŸš€ Starting Phase 3 Continuous Improvement Engine...")
        print("ðŸ” Monitoring quality metrics and triggering optimizations...")

        # Run self-healing cycle demonstration
        print("\nðŸ”„ Running self-healing cycle demonstration...")
        healing_results = await orchestrator.run_self_healing_cycle()

        print("\nâœ… Self-healing cycle completed:")
        print(f"ðŸ“Š Spectrums analyzed: {healing_results['spectrums_analyzed']}")
        print(f"ðŸ”§ Healing actions: {len(healing_results['healing_actions'])}")
        print(f"ðŸ“ˆ Improvements achieved: {len(healing_results['improvements_achieved'])}")
        print(f"ðŸ§  Learning patterns: {healing_results['learning_applied']}")
        print(f"â±ï¸  Duration: {healing_results['duration']:.1f}s")

        print("\nâœ… Phase 3 Continuous Improvement Engine demonstration completed")

    except Exception as e:
        logging.error(f"Phase 3 execution failed: {e}")
        raise


if __name__ == "__main__":
    asyncio.run(main())
