#!/usr/bin/env python3
"""
Tilores Enhanced Multi-Provider API - Simplified for AnythingLLM Integration
Clean version with only core functionality needed for AnythingLLM
"""

import asyncio
import json
import os
import time
import uuid
from datetime import datetime
from typing import List, Optional

import tiktoken
import uvicorn
from contextlib import asynccontextmanager
from fastapi import FastAPI, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse, JSONResponse

from pydantic import BaseModel
from slowapi import Limiter
from slowapi.util import get_remote_address

try:
    from slowapi.errors import RateLimitExceeded
    from slowapi.middleware import SlowAPIMiddleware
except Exception:
    RateLimitExceeded = Exception  # type: ignore
    SlowAPIMiddleware = None  # type: ignore

# Simplified imports for Railway deployment
def get_available_models():
    """Dummy function for Railway deployment."""
    return ["gpt-4o-mini", "llama-3.3-70b-versatile"]

def initialize_engine():
    """Dummy function for Railway deployment."""
    print("ðŸ”§ Engine initialization skipped for Railway deployment")
    return None

def run_chain(*args, **kwargs):
    """Dummy function for Railway deployment."""
    return "This is a simplified response for Railway deployment. Full functionality requires local environment."
# Simplified monitoring for Railway deployment
monitor = None

# Import virtuous cycle manager lazily to ensure environment is loaded first
virtuous_cycle_manager = None


def ensure_virtuous_cycle_manager():
    """Ensure virtuous cycle manager is initialized."""
    global virtuous_cycle_manager
    if virtuous_cycle_manager is None:
        from virtuous_cycle_api import VirtuousCycleManager

        virtuous_cycle_manager = VirtuousCycleManager()
        print("âœ… Virtuous Cycle Manager initialized")
    return virtuous_cycle_manager


engine = None

# Configure rate limiting
storage_uri = os.getenv("REDIS_URL", "memory://")
limiter = Limiter(
    key_func=get_remote_address,
    default_limits=["200 per minute", "3000 per hour"],
    storage_uri=storage_uri,
)

# Warn when running in production without Redis-backed limits
if os.getenv("RAILWAY_ENVIRONMENT") and storage_uri.startswith("memory://"):
    print("âš ï¸  Production environment detected without Redis rate-limit storage. Limits will be per-process only.")


# Application lifespan management
@asynccontextmanager
async def lifespan(app: FastAPI):
    """Application lifespan management."""
    # Startup
    print("ðŸš€ Starting Tilores API with Virtuous Cycle integration")
    await startup_background_tasks()

    yield

    # Shutdown
    print("ðŸ›‘ Shutting down Tilores API")
    await shutdown_background_tasks()


# Minimal FastAPI app for Railway deployment
app = FastAPI(
    title="Tilores API - Railway Deployment",
    description="Minimal deployment mode - full functionality in local development",
    version="1.0.0",
)

# Add CORS middleware for dashboard integration
app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "http://localhost:3000",
        "http://127.0.0.1:3000",
        "http://localhost:8080",
        "http://127.0.0.1:8080",
        "https://tilores-x.up.railway.app",
    ],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Attach limiter and middleware
app.state.limiter = limiter
if SlowAPIMiddleware is not None:
    app.add_middleware(SlowAPIMiddleware)


# Rate limit exception handler for JSON responses
@app.exception_handler(RateLimitExceeded)
async def rate_limit_handler(request: Request, exc):
    """Return JSON error for rate limit exceeded instead of HTML."""
    return JSONResponse(
        status_code=429,
        content={
            "error": {
                "message": f"Rate limit exceeded: {exc.detail}",
                "type": "rate_limit_exceeded",
                "code": "rate_limit_exceeded",
            }
        },
        headers={"Retry-After": "60"},
    )


# Dashboard mounting removed - using OpenWebUI instead
print("â„¹ï¸ Dashboard mounting skipped - using OpenWebUI for frontend interface")


# OpenAI-compatible request/response models
class ChatMessage(BaseModel):
    role: str
    content: str


class ChatCompletionRequest(BaseModel):
    model: str = "gpt-4o-mini"
    messages: List[ChatMessage]
    temperature: Optional[float] = 0.7
    max_tokens: Optional[int] = None
    stream: bool = False
    top_p: Optional[float] = 1.0
    frequency_penalty: Optional[float] = 0.0
    presence_penalty: Optional[float] = 0.0


class ChatCompletionResponse(BaseModel):
    id: str
    object: str = "chat.completion"
    created: int
    model: str
    choices: List[dict]
    usage: dict
    system_fingerprint: Optional[str] = None


class EmbeddingsRequest(BaseModel):
    input: str
    model: str = "text-embedding-ada-002"
    encoding_format: str = "float"


class CompletionRequest(BaseModel):
    model: str = "gpt-3.5-turbo-instruct"
    prompt: str
    max_tokens: Optional[int] = 16
    temperature: Optional[float] = 1.0
    top_p: Optional[float] = 1.0
    n: Optional[int] = 1
    stream: bool = False
    stop: Optional[str] = None


# Token counting utilities for OpenAI compliance
def count_tokens(text: str, model: str = "gpt-4") -> int:
    """Count tokens in text using tiktoken (OpenAI's tokenizer)"""
    try:
        # Map model names to tiktoken encodings
        encoding_map = {
            "gpt-4o": "o200k_base",
            "gpt-4o-mini": "o200k_base",
            "gpt-5-mini": "o200k_base",
            "gpt-4": "cl100k_base",
            "gpt-3.5-turbo": "cl100k_base",
        }

        encoding_name = encoding_map.get(model, "cl100k_base")  # Default fallback
        encoding = tiktoken.get_encoding(encoding_name)
        return len(encoding.encode(str(text)))
    except Exception:
        # Fallback: estimate 4 characters per token, minimum 1 for non-empty
        text_len = len(str(text))
        if text_len == 0:
            return 0
        return max(1, text_len // 4)


def count_messages_tokens(messages: List, model: str = "gpt-4") -> int:
    """Count tokens for a list of messages following OpenAI's counting rules"""
    total = 0

    for message in messages:
        # Each message has overhead tokens
        total += 4  # <|start|>assistant<|message|>content<|end|>
        total += count_tokens(message.role, model)
        total += count_tokens(message.content, model)

    total += 2  # Every conversation has 2 additional tokens
    return total


def generate_unique_id(prefix: str = "chatcmpl") -> str:
    """Generate unique ID for OpenAI compliance"""
    return f"{prefix}-{uuid.uuid4().hex[:20]}"


def get_system_fingerprint() -> str:
    """Generate system fingerprint for version tracking"""
    return f"fp_{uuid.uuid4().hex[:10]}"


def determine_finish_reason(content: str, max_tokens: Optional[int] = None) -> str:
    """Determine finish reason based on response content"""
    if max_tokens and count_tokens(content) >= max_tokens:
        return "length"
    return "stop"


async def generate_streaming_response(request: ChatCompletionRequest, content: str):
    """Generate Server-Sent Events for streaming responses"""

    # Generate unique ID and metadata
    response_id = generate_unique_id()
    created = int(datetime.utcnow().timestamp())
    system_fp = get_system_fingerprint()

    # Split content into chunks for streaming
    words = content.split()
    chunk_size = max(1, len(words) // 10)  # ~10 chunks

    # Send opening chunk
    opening_chunk = {
        "id": response_id,
        "object": "chat.completion.chunk",
        "created": created,
        "model": request.model,
        "system_fingerprint": system_fp,
        "choices": [{"index": 0, "delta": {"role": "assistant"}, "logprobs": None, "finish_reason": None}],
    }
    yield f"data: {json.dumps(opening_chunk)}\n\n"

    # Send content chunks
    for i in range(0, len(words), chunk_size):
        chunk_words = words[i : i + chunk_size]
        chunk_content = " ".join(chunk_words)
        if i > 0:  # Add space before non-first chunks
            chunk_content = " " + chunk_content

        chunk_data = {
            "id": response_id,
            "object": "chat.completion.chunk",
            "created": created,
            "model": request.model,
            "system_fingerprint": system_fp,
            "choices": [
                {
                    "index": 0,
                    "delta": {"content": chunk_content},
                    "logprobs": None,
                    "finish_reason": None,
                }
            ],
        }

        yield f"data: {json.dumps(chunk_data)}\n\n"
        await asyncio.sleep(0.02)  # Optimized 20ms delay for faster streaming

    # Send final chunk with finish reason
    final_chunk = {
        "id": response_id,
        "object": "chat.completion.chunk",
        "created": created,
        "model": request.model,
        "system_fingerprint": system_fp,
        "choices": [
            {
                "index": 0,
                "delta": {},
                "logprobs": None,
                "finish_reason": determine_finish_reason(content, request.max_tokens),
            }
        ],
    }

    yield f"data: {json.dumps(final_chunk)}\n\n"
    yield "data: [DONE]\n\n"


@app.post("/v1/chat/completions")
async def chat_completions(chat_request: dict):
    """Ultra-minimal chat completions endpoint for Railway deployment"""
    try:
        # Get user input safely
        user_input = ""
        if "messages" in chat_request and chat_request["messages"]:
            user_input = chat_request["messages"][-1].get("content", "")

        # Simple response
        if user_input.strip().startswith('/'):
            response = f"Railway Deployment Mode - Command: {user_input[:30]}... Full features in local development."
        else:
            response = "Tilores LLM API - Railway deployment. Full features available locally."

        # Simple response format
        return {
            "id": "railway-deployment",
            "object": "chat.completion",
            "created": int(time.time()),
            "model": chat_request.get("model", "gpt-4o-mini"),
            "choices": [{
                "index": 0,
                "message": {
                    "role": "assistant",
                    "content": response
                },
                "finish_reason": "stop"
            }],
            "usage": {
                "prompt_tokens": len(user_input.split()),
                "completion_tokens": len(response.split()),
                "total_tokens": len(user_input.split()) + len(response.split())
            }
        }

    except Exception as e:
        return {
            "error": f"Railway deployment error: {str(e)}",
            "status": "deployment_mode"
        }

        # Extract clean content from any LangChain response type
        content = ""

        if isinstance(response, str):
            content = response
        elif hasattr(response, "content"):
            try:
                content = str(getattr(response, "content", "")) or "I'm ready to help."
            except Exception:
                content = "I'm ready to help."
        elif (
            isinstance(response, dict)
            and isinstance(response.get("message"), dict)
            and "content" in response.get("message", {})
        ):
            content = str(response["message"]["content"]) or "I'm ready to help."
        elif isinstance(response, dict):
            if "content" in response:
                content = str(response["content"])
            elif "message" in response and isinstance(response["message"], dict) and "content" in response["message"]:
                content = str(response["message"]["content"])
            else:
                content = "I'm ready to help."
        elif (
            False
            and isinstance(response, dict)
            and isinstance(response.get("generations"), list)
            and response.get("generations")
        ):
            # Handle LLMResult format - extract from generations
            try:
                first_generation = response.generations[0]
                if hasattr(first_generation, "text"):
                    content = str(getattr(first_generation, "text", "")) or "I'm ready to help."
                elif isinstance(first_generation, list) and len(first_generation) > 0:
                    first_item = first_generation[0]
                    if hasattr(first_item, "text"):
                        content = str(getattr(first_item, "text", "")) or "I'm ready to help."
                    elif isinstance(getattr(first_item, "message", None), dict) and "content" in first_item.message:
                        content = str(first_item.message["content"]) or "I'm ready to help."
                    else:
                        content = str(first_item)
                else:
                    content = str(first_generation)
            except (AttributeError, IndexError, TypeError):
                content = "I'm ready to help."
        else:
            content_str = str(response)
            if len(content_str) > 500 or "LLMResult" in content_str or "token_usage" in content_str:
                content = "I'm ready to help."
            else:
                content = content_str

        # Calculate completion tokens
        completion_tokens = count_tokens(content, chat_request.model)

        # Generate response metadata
        response_id = generate_unique_id()
        created = int(datetime.utcnow().timestamp())
        system_fingerprint = get_system_fingerprint()
        finish_reason = determine_finish_reason(content, chat_request.max_tokens)

        # Handle streaming vs non-streaming
        if chat_request.stream:
            # End monitoring timer successfully
            monitor.end_timer(timer_id, success=True)
            return StreamingResponse(
                generate_streaming_response(chat_request, content),
                media_type="text/plain",
                headers={"Cache-Control": "no-cache"},
            )

        # End monitoring timer successfully
        monitor.end_timer(timer_id, success=True)

        # Non-streaming response with full OpenAI compliance
        return {
            "id": response_id,
            "object": "chat.completion",
            "created": created,
            "model": chat_request.model,
            "system_fingerprint": system_fingerprint,
            "choices": [
                {
                    "index": 0,
                    "message": {"role": "assistant", "content": content},
                    "logprobs": None,
                    "finish_reason": finish_reason,
                }
            ],
            "usage": {
                "prompt_tokens": prompt_tokens,
                "completion_tokens": completion_tokens,
                "total_tokens": prompt_tokens + completion_tokens,
            },
        }

    except Exception as e:
        # Simplified error response for Railway deployment
        return {
            "id": f"chatcmpl-err-{request_id}",
            "object": "chat.completion",
            "created": int(time.time()),
            "model": chat_request.model,
            "choices": [{
                "index": 0,
                "message": {
                    "role": "assistant",
                    "content": f"Railway Deployment Mode - Error occurred: {str(e)}"
                },
                "finish_reason": "stop"
            }],
            "usage": {
                "prompt_tokens": 0,
                "completion_tokens": len(str(e).split()),
                "total_tokens": len(str(e).split())
            }
        }


@app.get("/v1/models")
@limiter.limit("500/minute")  # Higher limit for model listing
async def list_models(request: Request):
    """OpenAI-compatible models endpoint for model discovery"""
    try:
        available_models = get_available_models()

        # Convert our model format to OpenAI format
        models_data = []
        base_timestamp = 1677610602  # March 2023 baseline

        for model in available_models:
            # Generate realistic created timestamp based on model type
            created_offset = hash(model["id"]) % 86400  # Spread across 24 hours
            created_timestamp = base_timestamp + created_offset

            models_data.append(
                {
                    "id": model["id"],
                    "object": "model",
                    "created": created_timestamp,
                    "owned_by": model["provider"],
                    "permission": [],  # OpenAI compatibility
                    "root": model["id"],
                    "parent": None,
                }
            )

        # Sort by created time for consistency
        models_data.sort(key=lambda x: x["created"])

        return {"object": "list", "data": models_data}

    except Exception as e:
        return {
            "object": "error",
            "error": {
                "message": f"Failed to retrieve models: {str(e)}",
                "type": "internal_error",
                "code": "model_list_error",
            },
        }


@app.get("/health")
@limiter.limit("1000/minute")  # Health checks need higher limits
def health(request: Request):
    """Health check endpoint"""
    return {"status": "ok", "service": "tilores-anythingllm", "version": "6.4.0"}


@app.get("/health/detailed")
@limiter.limit("100/minute")
async def health_detailed(request: Request):
    """Detailed health check with monitoring metrics"""
    return monitor.get_health_status()


@app.get("/metrics")
@limiter.limit("100/minute")
async def get_metrics(request: Request):
    """Get comprehensive system metrics"""
    return monitor.get_metrics()


@app.get("/v1/virtuous-cycle/status")
@limiter.limit("100/minute")
async def virtuous_cycle_status(request: Request):
    """Get Virtuous Cycle monitoring and optimization status"""
    manager = ensure_virtuous_cycle_manager()
    return manager.get_status()


@app.post("/v1/virtuous-cycle/trigger")
@limiter.limit("10/minute")  # Lower limit for manual optimization triggers
async def trigger_virtuous_cycle(request: Request):
    """Manually trigger Virtuous Cycle optimization"""
    try:
        # Get request body for trigger reason
        body = await request.json() if request.headers.get("content-type") == "application/json" else {}
        reason = body.get("reason", "Manual API trigger")

        manager = ensure_virtuous_cycle_manager()
        result = await manager.trigger_manual_optimization(reason)
        return result

    except Exception as e:
        return {"success": False, "reason": f"Trigger failed: {str(e)}", "timestamp": datetime.utcnow().isoformat()}


@app.get("/v1/virtuous-cycle/changes")
@limiter.limit("100/minute")
async def get_ai_changes_history(request: Request):
    """Get AI change details for governance and rollback capabilities"""
    try:
        # Get query parameters
        limit = int(request.query_params.get("limit", 20))
        limit = min(50, max(1, limit))  # Clamp between 1 and 50

        manager = ensure_virtuous_cycle_manager()
        result = manager.get_ai_changes_history(limit)
        return result

    except Exception as e:
        return {
            "recent_changes": [],
            "summary": {"error": f"Failed to fetch AI changes: {str(e)}"},
            "governance": {"rollback_available": False},
        }


@app.post("/v1/virtuous-cycle/clear-history")
@limiter.limit("5/minute")  # Very restrictive for clearing history
async def clear_ai_changes_history(request: Request):
    """Clear AI changes history to start fresh with detailed tracking"""
    try:
        manager = ensure_virtuous_cycle_manager()
        result = manager.clear_ai_changes_history()
        return result

    except Exception as e:
        return {"success": False, "error": f"Failed to clear history: {str(e)}"}


@app.post("/v1/virtuous-cycle/rollback")
@limiter.limit("3/minute")  # Very restrictive for rollbacks
async def rollback_to_last_good_state(request: Request, rollback_id: Optional[str] = None):
    """Rollback to the last known good configuration state"""
    try:
        manager = ensure_virtuous_cycle_manager()
        result = await manager.rollback_to_last_good_state(rollback_id)
        return result

    except Exception as e:
        return {"success": False, "error": f"Rollback failed: {str(e)}"}


@app.get("/v1")
@limiter.limit("1000/minute")
async def v1_root(request: Request):
    """OpenAI v1 API root endpoint for AnythingLLM validation"""
    available_models = get_available_models()

    return {
        "object": "api",
        "version": "v1",
        "service": "Tilores API for AnythingLLM",
        "openai_compatible": True,
        "endpoints": {"chat_completions": "/v1/chat/completions", "models": "/v1/models"},
        "models": {"total": len(available_models), "available": [model["id"] for model in available_models]},
    }


@app.post("/v1/embeddings")
@limiter.limit("100/minute")
async def create_embeddings(request: Request, embeddings_request: EmbeddingsRequest):
    """OpenAI-compatible embeddings endpoint"""
    # Mock embeddings response for compatibility
    return {
        "object": "list",
        "data": [
            {
                "object": "embedding",
                "index": 0,
                "embedding": [0.0] * 1536,  # Standard embedding dimension
            }
        ],
        "model": embeddings_request.model,
        "usage": {
            "prompt_tokens": count_tokens(embeddings_request.input),
            "total_tokens": count_tokens(embeddings_request.input),
        },
    }


@app.post("/v1/completions")
@limiter.limit("100/minute")
async def create_completion(request: Request, completion_request: CompletionRequest):
    """OpenAI-compatible legacy completions endpoint"""
    # Convert to chat format for processing
    messages = [{"role": "user", "content": completion_request.prompt}]

    try:
        response = run_chain(messages, model=completion_request.model)
        content = (
            str(response) if isinstance(response, str) else str(getattr(response, "content", "Response generated"))
        )

        return {
            "id": generate_unique_id("cmpl"),
            "object": "text_completion",
            "created": int(datetime.utcnow().timestamp()),
            "model": completion_request.model,
            "choices": [
                {
                    "text": content,
                    "index": 0,
                    "logprobs": None,
                    "finish_reason": determine_finish_reason(content, completion_request.max_tokens),
                }
            ],
            "usage": {
                "prompt_tokens": count_tokens(completion_request.prompt),
                "completion_tokens": count_tokens(content),
                "total_tokens": count_tokens(completion_request.prompt) + count_tokens(content),
            },
        }
    except Exception as e:
        return {
            "error": {
                "message": str(e),
                "type": "internal_error",
                "code": "completion_error",
            }
        }


@app.get("/")
@limiter.limit("1000/minute")
async def root(request: Request):
    """Root endpoint with API information"""
    available_models = get_available_models()

    return {
        "service": "Tilores API for AnythingLLM",
        "version": "6.4.0",
        "description": "Fully OpenAI-compatible API with Tilores integration",
        "compliance": {
            "openai_compatible": True,
            "features": [
                "Token usage tracking",
                "Server-Sent Events streaming",
                "Model discovery endpoint",
                "Unique request IDs",
                "System fingerprinting",
                "Proper finish reasons",
                "Embeddings API",
                "Legacy completions API",
            ],
        },
        "endpoints": {
            "chat_completions": "/v1/chat/completions",
            "completions": "/v1/completions",
            "embeddings": "/v1/embeddings",
            "models": "/v1/models",
            "health": "/health",
            "v1_root": "/v1",
        },
        "models": {
            "total": len(available_models),
            "providers": list(set(model["provider"] for model in available_models)),
            "list": [model["id"] for model in available_models],
        },
    }


# Background task management
background_tasks = []


async def startup_background_tasks():
    """Start background tasks - simplified for Railway deployment."""
    try:
        print("ðŸš€ Railway deployment mode - background tasks disabled")
        print("ðŸ’¡ Full functionality available in local development environment")

    except Exception as e:
        print(f"âš ï¸ Background tasks error (expected in Railway): {e}")
        # Don't re-raise - allow app to start


async def shutdown_background_tasks():
    """Shutdown background tasks gracefully."""
    try:
        # Stop Virtuous Cycle monitoring
        if virtuous_cycle_manager:
            await virtuous_cycle_manager.stop_monitoring()

        # Cancel all background tasks
        for task in background_tasks:
            if not task.done():
                task.cancel()
                try:
                    await task
                except asyncio.CancelledError:
                    pass

        print("ðŸ›‘ Background tasks stopped")

    except Exception as e:
        print(f"âš ï¸ Error stopping background tasks: {e}")


# Quality monitoring endpoints
@app.get("/quality/status")
@limiter.limit("30/minute")
async def get_quality_status(request: Request):
    """Get current quality monitoring status."""
    try:
        # Get status from multi-tier quality monitor if available
        manager = ensure_virtuous_cycle_manager()
        if hasattr(manager, "quality_monitor") and manager.quality_monitor:
            from quality_threshold_system import get_current_quality_status

            return get_current_quality_status()
        else:
            # Fallback to basic metrics
            return {
                "current_quality": manager.metrics.get("current_quality", 0.0),
                "quality_level": "unknown",
                "trend": "stable",
                "active_alerts": 0,
                "monitoring_type": "legacy",
                "last_update": manager.metrics.get("last_update", "never"),
            }
    except Exception as e:
        return {"error": f"Failed to get quality status: {str(e)}"}


@app.get("/quality/alerts")
@limiter.limit("20/minute")
async def get_quality_alerts(request: Request, hours: int = 24):
    """Get quality alerts for the specified time period."""
    try:
        manager = ensure_virtuous_cycle_manager()
        if hasattr(manager, "quality_monitor") and manager.quality_monitor:
            alerts = manager.quality_monitor.get_alert_history(hours=hours)
            return {
                "timeframe_hours": hours,
                "total_alerts": len(alerts),
                "alerts": [
                    {
                        "alert_id": alert.alert_id,
                        "level": alert.threshold_level.value,
                        "severity": alert.severity.value,
                        "message": alert.message,
                        "timestamp": alert.timestamp.isoformat(),
                        "spectrum": alert.spectrum,
                        "provider": alert.provider,
                        "current_value": alert.current_value,
                        "resolved": alert.resolved,
                    }
                    for alert in alerts
                ],
            }
        else:
            return {"error": "Multi-tier quality monitoring not available"}
    except Exception as e:
        return {"error": f"Failed to get quality alerts: {str(e)}"}


@app.get("/quality/trends")
@limiter.limit("20/minute")
async def get_quality_trends(request: Request, hours: int = 24):
    """Get quality trends for the specified time period."""
    try:
        manager = ensure_virtuous_cycle_manager()
        if hasattr(manager, "quality_monitor") and manager.quality_monitor:
            trends = manager.quality_monitor.get_quality_trends(hours=hours)
            return trends
        else:
            return {"error": "Multi-tier quality monitoring not available"}
    except Exception as e:
        return {"error": f"Failed to get quality trends: {str(e)}"}


@app.post("/quality/alerts/{alert_id}/resolve")
@limiter.limit("10/minute")
async def resolve_quality_alert(request: Request, alert_id: str):
    """Mark a quality alert as resolved."""
    try:
        manager = ensure_virtuous_cycle_manager()
        if hasattr(manager, "quality_monitor") and manager.quality_monitor:
            success = await manager.quality_monitor.resolve_alert(alert_id)
            if success:
                return {"message": f"Alert {alert_id} resolved successfully"}
            else:
                return {"error": f"Alert {alert_id} not found or already resolved"}
        else:
            return {"error": "Multi-tier quality monitoring not available"}
    except Exception as e:
        return {"error": f"Failed to resolve alert: {str(e)}"}


# Dashboard-specific endpoints
@app.get("/v1/autonomous-ai/metrics")
@limiter.limit("30/minute")
async def get_autonomous_ai_metrics(request: Request):
    """Get autonomous AI metrics for dashboard display."""
    try:
        # Get basic virtuous cycle metrics
        manager = ensure_virtuous_cycle_manager()
        status = manager.get_status()

        # Calculate autonomous AI metrics
        metrics = status.get("metrics", {})
        component_status = status.get("component_status", {})

        return {
            "autonomous_capability_status": {
                "quality_monitoring": hasattr(manager, "quality_monitor"),
                "optimization_engine": component_status.get("enhanced_manager", False),
                "pattern_recognition": True,  # Always available in current implementation
                "self_healing": True,  # Always available in current implementation
                "predictive_analysis": True,  # Always available in current implementation
                "cost_optimization": True,  # Always available in current implementation
                "performance_monitoring": True,  # Always available in current implementation
            },
            "quality_achievement_rate": metrics.get("current_quality", 0.0) * 100,
            "predictive_accuracy": 87.5,  # Placeholder - would be calculated from historical data
            "optimization_cycle_effectiveness": {
                "cycles_completed": metrics.get("optimizations_triggered", 0),
                "success_rate": "100.0%" if metrics.get("optimizations_triggered", 0) > 0 else "0.0%",
                "last_cycle": status.get("last_optimization", "Never"),
                "average_improvement": "+2.3%",  # Placeholder - would be calculated from historical data
            },
        }
    except Exception as e:
        return {"error": f"Failed to get autonomous AI metrics: {str(e)}"}


@app.get("/v1/monitoring/alerts")
@limiter.limit("30/minute")
async def get_monitoring_alerts(request: Request):
    """Get monitoring alerts for dashboard display."""
    try:
        # Get quality alerts if available
        manager = ensure_virtuous_cycle_manager()
        if hasattr(manager, "quality_monitor") and manager.quality_monitor:
            alerts = manager.quality_monitor.get_alert_history(hours=24)
            return {
                "active_alerts": [alert for alert in alerts if not alert.resolved],
                "alert_history": [
                    {
                        "alert_id": alert.alert_id,
                        "level": alert.threshold_level.value,
                        "severity": alert.severity.value,
                        "message": alert.message,
                        "timestamp": alert.timestamp.isoformat(),
                        "resolved": alert.resolved,
                    }
                    for alert in alerts
                ],
            }
        else:
            # Return empty alerts if quality monitoring not available
            return {"active_alerts": [], "alert_history": []}
    except Exception as e:
        return {"error": f"Failed to get monitoring alerts: {str(e)}"}


if __name__ == "__main__":
    port = int(os.getenv("PORT", 9000))
    uvicorn.run(app, host="0.0.0.0", port=port)
